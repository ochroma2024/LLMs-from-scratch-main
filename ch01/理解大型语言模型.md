# 本章涵盖
 1.神经网络架构
 LLM通常采用DNN架构，最常见的是基于Transformer架构的模型。通过自注意力机制来处理输入数据，使模型能够捕捉长距离的依赖关系和复杂的上下文信息。
 2.自注意力机制
 自注意力机制使模型能够在处理输入序列时对序列中的每个位置计算注意力权重，从而关注到不同位置的重要性。这种机制使得模型能够更好地理解上下文，尤其是处理长文本时。
 3.预训练和微调
 大型语言模型的训练通常分为两个阶段：
 预训练 pre-training ：模型在大规模的未标注文本数据上进行无监督学习，通过预测下一个单词或填补背遮盖的单词来学习语言的统计特征和语义信息。
 微调 Fine-tuning ：在特定任务或领域的数据上对预训练好的模型进行有监督学习，以适应具体任务的需求，如文本分类、机器翻译等。
 4.大规模数据和计算资源
 训练大型语言模型需要海量的文本数据和强大的计算资源。大规模的数据提供了丰富的语言知识，而强大的计算资源则使得模型能够处理复杂的计算任务。
 **5.词嵌入（Word Embeddings）**
 词嵌入是将词语表示为连续向量的一种技术，使得词语之间的语义关系能够在向量空间中表达出来。常见的词嵌入技术包括Word2Vec、GloVe和BERT等。大型语言模型通过词嵌入将文本转换为向量表示，从而进行进一步的处理。
 6.生成式和判别式模型
 大型语言模型可以分为**生成式和判别式**两类：
 生成式模型：如GPT系列，能够生成连续的自然语言文本，适用于文本生成、对话系统等任务。
 判别式模型：如BERT，主要用于判别任务，如文本分类、问答系统等，通过判断输入文本是否符合特定的模式或要求